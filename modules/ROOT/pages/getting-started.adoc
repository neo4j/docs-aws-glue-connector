= Getting Started

== Installation

=== Upload the jar and connection details
1. Download the most recent version of the connector from link:https://github.com/neo4j/neo4j-aws-glue/releases[AWS Glue connector - GitHub]
2. Upload the connector, `neo4j-aws-glue-<version>.jar` file to Amazon S3 bucket
3. Store the connection details for Neo4j or AuraDB in AWS Secret Manager as key/value pairs for _user_ and _password_
4. Record/copy the Secret ARN as this will be required later when using the secret from the connection

[NOTE]
====
Instead of using the naming from the environment file (_neo4j_user_ and _neo4j_password_) this guide uses _user_ and _password_.
====

=== Create a custom connector 
In the AWS Glue console

1. Select _Create custom connector_
2. Give the connector a name e.g. `Neo4j-Connector-for-AWS-Glue`
3. For _Connector S3 URL_ provide the link to the connector in the S3 bucket (from above)
4. Provide the class name `org.neo4j.jdbc.Neo4jJdbcDriver`
5. Provide the following JDBC base URL `jdbc:neo4j+s://<aura>?enableSQLTranslation=true` to enable the JDBC driver's built-in translator feature to translate SQL commands to Cypher
6. Set the URL parameter delimiter to `&`

=== Creating a connection

A connection can only be created after you have created the custom connector in the previous step.
It is essential to use the key/value pairs of _user_ and _password_ provided in the AWS Secrets Manager because those properties must be used for the actual connection. 

1. Provide a connection name e.g. `aws-glue-connection-to-neo4j-auradb`
2. For connection credential type, select _<default>_
3. For AWS Secret, select _Secret from this account_
4. Provide the name of the secret used to store the credentials above
5. Expand Additional Options and provide the following additional parameters as pairs

|====
|Key|Property
|user|$\{user}
|password|$\{password}
|====

== Quickstart
The following Quickstarts demonstrate how to use the AWS Glue Connector to perform an import of **TODO** <name of data set> <available here> **TODO** and export a subset of the data to Parquet files on S3 using the Visual ETL Editor.
The Quickstarts assume the Getting Started guide has been used to install the connector and configure a connection to the database, please change the connection name accordingly. 

=== Create the blueprint Schema in AuraDB/Neo4j
1. Use Query or Browser to create a blueprint schema that describes the nodes, relationships and properties that will be imported.

[WARNING]
====
**TODO**
```
Cypher statements to create nodes, relationships, properties
```
====

=== Importing nodes into AuraDB/Neo4j
1. In the AWS Glue console, select Visual ETL, title the job e.g. Import data from Parquet on S3 into AuraDB
2. Select Amazon S3 from the list of Sources, and select S3 source type, S3 location and Browse to S3 to locate the files for import
3. Select the Parquet files containing the nodes for import first
4. Select _Infer schema_
5. From the list of targets select `Neo4j-Connector-for-AWS-Glue`
6. For the Target properties provide the `aws-glue-connection-to-neo4j-auradb`
7. The table name should be set to the name of the labels that are to be imported e.g. `Movie`.
8. The schema will be displayed, if you want to drop any columns then add a Transform - DropFields steps (or other suitable transformations).
If this data is from another Neo4j database then you should drop the `v$id`.
9. Add flows for each set of import files and table names. For nodes these may be added in parallel.
10. Start the import job.

=== Importing relationships into AuraDB/Neo4j
Once the nodes have been imported you can import the relationships. 
You can add these as additional steps after the steps to import the nodes i.e. in series OR create a new import job.

1. Select the Parquet files containing the nodes for import first
2. Select _Infer schema_
3. From the list of targets select `Neo4j-Connector-for-AWS-Glue`
4. For the Target properties provide the `aws-glue-connection-to-neo4j-auradb`

[WARNING]
====
**TODO**
<need details on creating the custom code block>
====

```python
def MyTransform (glueContext, dfc) -> DynamicFrameCollection:
    # ==============================================================================
    # AWS Glue Custom Transformation Script for Neo4j
    # This script defines a function to be used in a Visual ETL job.
    # It connects to Neo4j and executes Cypher queries as a side-effect.
    #
    # NOTE: The password is hardcoded for initial testing. For production,
    # it is highly recommended to retrieve the password from a secure source
    # like AWS Secrets Manager.
    # ==============================================================================
    
    import sys
    from neo4j import GraphDatabase
    from awsglue.transforms import *
    from awsglue.utils import getResolvedOptions
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    from awsglue.job import Job
    
    # Hardcoded Neo4j connection details
    NEO4J_USER = 'neo4j'
    NEO4J_URI = 'neo4j+s://auradb.databases.neo4j.io'
    NEO4J_PASSWORD = 'Passwordâ€™
    
    def execute_cypher_queries(driver, query_list):
    """
    Executes a list of Cypher queries within a single Neo4j session.
    """
    with driver.session() as session:
        for query in query_list:
            try:
                session.run(query)
                print(f"Successfully executed query: {query.strip().splitlines()[0]}...")
            except Exception as e:
                print(f"Error executing query: {query}\nError: {e}")
                raise
    
    def neo4j_cypher_transformation(dynamic_frame):
    """
    This is the main function for the custom transformation.
    It will be called by the AWS Glue Visual ETL job.
    
    Args:
        dynamic_frame: The input DynamicFrame from the previous step.
        
    Returns:
        The original DynamicFrame, as no data manipulation is performed.
    """
    print("Starting Neo4j Cypher transformation...")
    
    try:        
        # Define the Cypher queries to be executed
        cypher_queries = [
            """
            MATCH (j:OrderCustomerJoin)
            MATCH (c:Customer {customer_id: j.customer_id})
            MATCH (o:Order {order_id: j.order_id})
            MERGE (c)-[:ORDERED]->(o);
            """,
            """
            MATCH (j:OrderCustomerJoin) DELETE j;
            """
        ]
        
        # Connect to Neo4j and execute queries using the hardcoded password
        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
        print("Successfully connected to Neo4j AuraDB.")
        
        execute_cypher_queries(driver, cypher_queries)
        
    except Exception as e:
        print(f"An error occurred in the transformation: {e}")
        # In a real-world scenario, you might want to handle this more gracefully
        # or log the error before re-raising.
        raise e
        
    finally:
        if 'driver' in locals() and driver:
            driver.close()
            print("Neo4j driver connection closed.")
            
    print("Neo4j Cypher transformation completed.")
    
    # This transformation does not modify the data, so it returns the original DynamicFrame.
    return dynamic_frame
    
    # Note: The glue job will automatically call the function you define
    # when you configure the custom transformation in Visual ETL.
```

=== Exporting from AuraDB/Neo4j
1. In the AWS Glue console, select Visual ETL, title the job e.g. _Export data from AuraDB to Parquet on S3_
2. Select `Neo4j-Connector-for-AWS-Glue` from the list of Sources
3. For the Target properties provide the `aws-glue-connection-to-neo4j-auradb`
4. The Table name should be set to the name of the labels that are to be imported e.g. `Movie`
5. The _Data preview_ should populate with a sample of the data
6. Browse the list of targets and select S3 source type, S3 location and browse to S3 location for storing the Parquet files
7. The schema will be displayed, if you want to drop any columns then add a Transform - DropFields steps (or other suitable transformations).
If this data is from another Neo4j database then you should drop the `v$id`
8. Add flows for each set of import files and table names. For nodes these may be added in parallel.
9. Start the import job.

