= Getting Started

== Installation

=== Upload the jar and connection details
. Download the most recent version of the connector from link:https://github.com/neo4j/neo4j-aws-glue/releases[AWS Glue connector - GitHub].
. Upload the connector, `neo4j-aws-glue-<version>.jar` file to Amazon S3 bucket.
. Store the connection details for Neo4j or AuraDB in AWS Secret Manager as key/value pairs for _user_ and _password_.
. Record/copy the Secret ARN as this will be required later when using the secret from the connection.

[NOTE]
====
Instead of using the naming from the environment file (_neo4j_user_ and _neo4j_password_) this guide uses _user_ and _password_.
====

=== Create a custom connector 
In the AWS Glue console

. Select _Create custom connector_.
. Give the connector a name. For example `Neo4j-Connector-for-AWS-Glue`.
. For _Connector S3 URL_ provide the link to the connector in the S3 bucket (from above).
. Provide the class name `org.neo4j.jdbc.Neo4jJdbcDriver`.
. Provide the following JDBC base URL `jdbc:neo4j+s://<aura>?enableSQLTranslation=true` to enable the JDBC driver's built-in translator feature to translate SQL commands to Cypher.
. Set the URL parameter delimiter to `&`.

=== Creating a connection

A connection can only be created after you have created the custom connector in the previous step.
It is essential to use the key/value pairs of _user_ and _password_ provided in the AWS Secrets Manager because those properties must be used for the actual connection. 

. Provide a connection name. For example `aws-glue-connection-to-neo4j-auradb`.
. For connection credential type, select _<default>_.
. For AWS Secret, select _Secret from this account_.
. Provide the name of the secret used to store the credentials above.
. Expand Additional Options and provide the following additional parameters as pairs.

|====
|Key|Property
|user|$\{user}
|password|$\{password}
|====

== Quickstart
The following Quickstarts demonstrate how to use the AWS Glue Connector to perform an import of a virtual data set and export a subset of the data to Parquet files on S3 using the Visual ETL Editor.
The Quickstarts assume the Getting Started guide has been used to install the connector and configure a connection to the database, please change the connection name accordingly. 

=== Create the blueprint Schema in AuraDB/Neo4j
Use Query or Browser to create a blueprint schema that describes the nodes, relationships and properties that will be imported.

====
[source,java]
```
CREATE (m:Movie{title:'The Matrix', released:1999})
CREATE (p:Person{name:'Keanu Reeves', born:'1964'})
CREATE (p)-[:ACTED_IN{roles: ['Neo']}]->(m)
```
====

=== Importing nodes into AuraDB/Neo4j
. In the AWS Glue console, select *Visual ETL*, title the job (for example "Import data from Parquet on S3 into AuraDB").
. Select *Amazon S3* from the list of sources, and select *S3 source type*, *S3 location* and *Browse to S3* to locate the files for import.
. Select the Parquet files containing the nodes for import first.
. Select _Infer schema_.
. From the list of targets select `Neo4j-Connector-for-AWS-Glue`.
. For the target properties provide the `aws-glue-connection-to-neo4j-auradb`.
. The table name should be set to the name of the labels that are to be imported, for example `Movie`.
. The schema will be displayed, if you want to drop any columns then add a *Transform - DropFields* step (or other suitable transformations).
If this data is from another Neo4j database then you should drop the `v$id`.
. Add flows for each set of import files and table names. For nodes these may be added in parallel.
. Start the import job.

=== Importing relationships into AuraDB/Neo4j
Once the nodes have been imported you can import the relationships. 
You can add these as additional steps after the steps to import the nodes, for example in series OR create a new import job.

. In the AWS Glue console, select *Visual ETL* and give a name to the job (for example "Import relationship data from Parquet on S3 into AuraDB").
. Select *Amazon S3* from the list of sources, then select *S3 source type* -> *S3 location* and *Browse to S3* to locate the files to import.
. Select the Parquet files containing the nodes and relationships to import first.
. Use a transformer such as *SQL Query* to create the needed output pattern, which in this example is`(v$person_id, roles, v$movie_id`). 
. From the list of targets select `Neo4j-Connector-for-AWS-Glue`.
. For the target properties provide the `aws-glue-connection-to-neo4j-auradb`.
. The table name should be set to the (virtual) relationship table as `Person_ACTED_IN_Movie`.
. Start the import job.

=== Exporting from AuraDB/Neo4j
. In the AWS Glue console, select Visual ETL, title the job, for example _Export data from AuraDB to Parquet on S3_.
. Select `Neo4j-Connector-for-AWS-Glue` from the list of *Sources*.
. For the Target properties provide the `aws-glue-connection-to-neo4j-auradb`.
. The Table name should be set to the name of the labels that are to be imported, for example `Movie`.
. The _Data preview_ should populate with a sample of the data.
. Browse the list of targets and select *S3 source type*, *S3 location* and browse to *S3 location* for storing the Parquet files.
. The schema will be displayed, if you want to drop any columns then add a *Transform - DropFields* steps (or other suitable transformations).
If this data is from another Neo4j database then you should drop the `v$id`.
. Add flows for each set of import files and table names. For nodes these may be added in parallel.
. Start the import job.

